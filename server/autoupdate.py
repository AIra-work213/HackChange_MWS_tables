# -*- coding: utf-8 -*-
"""–∞–≤—Ç–æ–æ–±–Ω–æ–≤–∞

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hk_1HcQ4Z7yOjMQF-1f0gDhrqw4qS9RO
"""

import requests
import pandas as pd
import numpy as np
from tqdm import tqdm
import time
import logging
from datetime import datetime, timedelta
import warnings
import concurrent.futures
from threading import Lock
import random

# –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore', category=FutureWarning)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.WARNING)

# –°–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Å–≤—è–∑–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (ID -> –Ω–∞–∑–≤–∞–Ω–∏–µ)
VK_GROUPS = {
    '-8458649': '–ú–¢–°',    # –ú–¢–°
    '-26514504': '–ë–∏–ª–∞–π–Ω', # –ë–∏–ª–∞–π–Ω
    '-18098621': '–¢–µ–ª–µ2',  # –¢–µ–ª–µ2
    '-50353992': '–ô–æ—Ç–∞',   # –ô–æ—Ç–∞
    '-3785': '–ú–µ–≥–∞—Ñ–æ–Ω'     # –ú–µ–≥–∞—Ñ–æ–Ω
}

VK_TOKEN = '954e97ed954e97ed954e97ed2d967306909954e954e97edfc65b931ccd5ae0f3c2a7afe'

# MWS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
MWS_URL = "https://tables.mws.ru/fusion/v1/datasheets/dstRtSXBewJh8lLCNz/records"
MWS_HEADERS = {
    "Authorization": "Bearer uskSID2MFKEnL7AVNUdLrnn",
    "Content-Type": "application/json"
}
MWS_PARAMS = {
    "viewId": "viwYvxvon7TBU",
    "fieldKey": "name"
}

# –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É
MAX_POSTS_PER_GROUP = 8000

# –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
LAST_UPDATE_RECORD_ID = "last_update_tracker"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
success_count = 0
error_count = 0
counter_lock = Lock()

class FastVKDataCollector:
    def __init__(self, vk_token, api_version='5.199'):
        self.vk_token = vk_token
        self.api_version = api_version
        self.base_url = 'https://api.vk.com/method/'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def get_wall_posts_fast(self, owner_id, offset=0, count=100):
        """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –±–µ–∑ —Ä–µ—Ç—Ä–∞–µ–≤"""
        time.sleep(0.05)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

        method = 'wall.get'
        params = {
            'access_token': self.vk_token,
            'owner_id': owner_id,
            'v': self.api_version,
            'offset': offset,
            'count': count,
            'extended': 0
        }

        try:
            response = self.session.get(f"{self.base_url}{method}", params=params, timeout=5)
            response.raise_for_status()
            data = response.json()

            if 'error' in data:
                error_code = data['error'].get('error_code')
                if error_code in [6, 9, 29]:  # Rate limit –æ—à–∏–±–∫–∏
                    time.sleep(1)
                return None

            return data

        except Exception:
            return None

    def calculate_er(self, likes, reposts, comments, views):
        if not views or views == 0:
            return 0
        return ((likes + reposts + comments) / views) * 100

    def categorize_efficiency(self, er):
        if er >= 8:
            return "–í—ã—Å–æ–∫–∞—è"
        elif er >= 4:
            return "–°—Ä–µ–¥–Ω—è—è"
        elif er >= 2:
            return "–ù–∏–∑–∫–∞—è"
        else:
            return "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"

    def extract_post_data_fast(self, post, group_name):
        try:
            post_id = post.get('id')
            owner_id = post.get('owner_id')
            date_timestamp = post.get('date')
            text = post.get('text', '')

            likes = post.get('likes', {}).get('count', 0)
            reposts = post.get('reposts', {}).get('count', 0)
            comments = post.get('comments', {}).get('count', 0)
            views = post.get('views', {}).get('count') if post.get('views') else 0

            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = ''
            for attachment in post.get('attachments', [])[:1]:
                attachment_type = attachment.get('type')
                if attachment_type in ['link', 'video']:
                    attachment_data = attachment.get(attachment_type, {})
                    title = attachment_data.get('title', '')[:200]
                    break

            er = self.calculate_er(likes, reposts, comments, views)
            efficiency = self.categorize_efficiency(er)

            return {
                'id_group': owner_id,
                'group_name': group_name,
                'id_post': post_id,
                'date_time': date_timestamp,
                'title': title,
                'text': text[:500],
                'views': views,
                'likes': likes,
                'reposts': reposts,
                'comments_count': comments,
                'ER': round(er, 2),
                'Efficiency': efficiency,
                'day_of_week': ["–ü–Ω", "–í—Ç", "–°—Ä", "–ß—Ç", "–ü—Ç", "–°–±", "–í—Å"][datetime.fromtimestamp(date_timestamp).weekday()],
                'date': datetime.fromtimestamp(date_timestamp).strftime('%Y-%m-%d'),
                'time_period': self.get_time_period_fast(date_timestamp),
                'len_text': len(text)
            }

        except Exception:
            return None

    def get_time_period_fast(self, timestamp):
        hour = datetime.fromtimestamp(timestamp).hour
        if 6 <= hour < 12:
            return "–£—Ç—Ä–æ"
        elif 12 <= hour < 18:
            return "–î–µ–Ω—å"
        elif 18 <= hour < 24:
            return "–í–µ—á–µ—Ä"
        else:
            return "–ù–æ—á—å"

def get_last_update_date():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    try:
        params = {
            **MWS_PARAMS,
            "recordIds": LAST_UPDATE_RECORD_ID
        }

        response = requests.get(
            MWS_URL,
            params=params,
            headers=MWS_HEADERS,
            timeout=10
        )

        if response.status_code == 200:
            data = response.json()
            if data and 'data' in data and data['data']:
                record = data['data'][0]
                post_date_time = record.get('fields', {}).get('post_date_time')
                if post_date_time:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥ –≤ —Å–µ–∫—É–Ω–¥—ã
                    timestamp = post_date_time / 1000
                    last_update_date = datetime.fromtimestamp(timestamp)
                    print(f"üìÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–µ: {last_update_date}")
                    return last_update_date

        # –ï—Å–ª–∏ –∑–∞–ø–∏—Å—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        print("‚ö†Ô∏è –ó–∞–ø–∏—Å—å –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 11 –æ–∫—Ç—è–±—Ä—è 2025")
        return datetime(2025, 10, 11)

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 11 –æ–∫—Ç—è–±—Ä—è 2025")
        return datetime(2025, 10, 11)

def upload_batch_to_mws(posts_data):
    global success_count, error_count

    if not posts_data:
        return 0

    records = []
    for post in posts_data:
        records.append({
            "post_id": str(post['id_post']),
            "title": str(post['title']),
            "text": str(post['text']),
            "attachment_description": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            "views": int(post['views']),
            "likes": int(post['likes']),
            "reposts": int(post['reposts']),
            "comments_count": int(post['comments_count']),
            "post_date_time": int(post['date_time']) * 1000,
            "owner_id": str(post['group_name']),  # –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –≤–º–µ—Å—Ç–æ ID
            "ER": float(post['ER']),
            "Efficiency": str(post['Efficiency']),
            "day_of_week": str(post['day_of_week']),
            "post_date": int(post['date_time']) * 1000,
            "time_period": str(post['time_period']),
            "len_text": int(post['len_text'])
        })

    data = {
        "records": [{'fields': record} for record in records],
        "fieldKey": "name"
    }

    try:
        response = requests.post(
            MWS_URL,
            params=MWS_PARAMS,
            headers=MWS_HEADERS,
            json=data,
            timeout=10
        )

        if response.status_code in [200, 201]:
            with counter_lock:
                success_count += len(posts_data)
            return len(posts_data)
        else:
            # –ï—Å–ª–∏ –±–∞—Ç—á –Ω–µ –ø—Ä–æ—à–µ–ª, –ø—Ä–æ–±—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É
            successful = 0
            for post in posts_data:
                if upload_single_post_fast(post):
                    successful += 1
            return successful

    except Exception:
        # Fallback –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
        successful = 0
        for post in posts_data:
            if upload_single_post_fast(post):
                successful += 1
        return successful

def upload_single_post_fast(post_data):
    global success_count, error_count

    try:
        record = {
            "post_id": str(post_data['id_post']),
            "title": str(post_data['title']),
            "text": str(post_data['text']),
            "attachment_description": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            "views": int(post_data['views']),
            "likes": int(post_data['likes']),
            "reposts": int(post_data['reposts']),
            "comments_count": int(post_data['comments_count']),
            "post_date_time": int(post_data['date_time']) * 1000,
            "owner_id": str(post_data['group_name']),  # –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –≤–º–µ—Å—Ç–æ ID
            "ER": float(post_data['ER']),
            "Efficiency": str(post_data['Efficiency']),
            "day_of_week": str(post_data['day_of_week']),
            "post_date": int(post_data['date_time']) * 1000,
            "time_period": str(post_data['time_period']),
            "len_text": int(post_data['len_text'])
        }

        data = {
            "records": [{'fields': record}],
            "fieldKey": "name"
        }

        response = requests.post(
            MWS_URL,
            params=MWS_PARAMS,
            headers=MWS_HEADERS,
            json=data,
            timeout=5
        )

        if response.status_code in [200, 201]:
            with counter_lock:
                success_count += 1
            return True
        else:
            error_text = response.text.lower()
            if "duplicate" in error_text or "—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç" in error_text:
                with counter_lock:
                    success_count += 1
                return True
            return False

    except Exception:
        with counter_lock:
            error_count += 1
        return False

def find_start_offset(collector, owner_id, total_posts, cutoff_date):
    """–ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –¥–æ cutoff –¥–∞—Ç—ã"""
    print("üîç –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏...")

    low = 0
    high = min(total_posts, 2000)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ–∏—Å–∫ –ø–µ—Ä–≤—ã–º–∏ 2000 –ø–æ—Å—Ç–∞–º–∏
    cutoff_timestamp = cutoff_date.timestamp()

    while low <= high:
        mid = (low + high) // 2
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ offset {mid}...")

        data = collector.get_wall_posts_fast(owner_id, mid, 1)
        if not data or not data['response']['items']:
            break

        post = data['response']['items'][0]
        post_timestamp = post.get('date', 0)

        if post_timestamp > cutoff_timestamp:
            low = mid + 1
        else:
            high = mid - 1

    # –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞–π–¥–µ–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
    start_offset = max(0, low - 50)  # –ù–µ–º–Ω–æ–≥–æ –æ—Ç—Å—Ç—É–ø–∞–µ–º –Ω–∞–∑–∞–¥ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    print(f"–ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Ç—Å—Ç—É–ø–∞: {start_offset}")
    return start_offset

def collect_group_posts_optimized(owner_id, group_name, cutoff_date):
    global success_count, error_count

    collector = FastVKDataCollector(VK_TOKEN)

    print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã: {group_name} ({owner_id})")

    # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    test_data = collector.get_wall_posts_fast(owner_id, 0, 1)
    if not test_data:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä—É–ø–ø—ã {group_name}")
        return 0

    total_posts = test_data['response']['count']
    print(f"üìà –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {total_posts}")

    if total_posts == 0:
        return 0

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é
    start_offset = find_start_offset(collector, owner_id, total_posts, cutoff_date)

    collected = 0
    offset = start_offset
    batch_size = 100
    cutoff_timestamp = cutoff_date.timestamp()

    with tqdm(total=MAX_POSTS_PER_GROUP, desc=f"–ì—Ä—É–ø–ø–∞ {group_name}") as pbar:
        while collected < MAX_POSTS_PER_GROUP:
            # –±–∞—Ç—á
            data = collector.get_wall_posts_fast(owner_id, offset, batch_size)
            if not data:
                break

            posts = data['response'].get('items', [])
            if not posts:
                break

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å—Ç—ã
            valid_posts = []
            for post in posts:
                if collected >= MAX_POSTS_PER_GROUP:
                    break

                post_timestamp = post.get('date', 0)
                if post_timestamp <= cutoff_timestamp:
                    post_data = collector.extract_post_data_fast(post, group_name)
                    if post_data:
                        valid_posts.append(post_data)
                        collected += 1
                        pbar.update(1)
                else:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã
                    collected += 1
                    pbar.update(1)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –ø–æ—Å—Ç—ã –±–∞—Ç—á–µ–º
            if valid_posts:
                upload_batch_to_mws(valid_posts)

            offset += batch_size

            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –≤—Å–µ –ø–æ—Å—Ç—ã –≤ –±–∞—Ç—á–µ –Ω–æ–≤—ã–µ, —É—Å–∫–æ—Ä—è–µ–º—Å—è
            if len(valid_posts) == 0 and len(posts) > 0:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à–∞–≥ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
                skip_ahead = min(500, MAX_POSTS_PER_GROUP - collected)
                if skip_ahead > 100:
                    offset += skip_ahead - batch_size
                    pbar.update(skip_ahead)
                    collected += skip_ahead
                    print(f"‚ö° –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {skip_ahead} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤")

    print(f"‚úÖ –ì—Ä—É–ø–ø–∞ {group_name}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {collected} –ø–æ—Å—Ç–æ–≤")
    return collected

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    global success_count, error_count

    print("üöÄ –ó–ê–ü–£–°–ö –°–ë–û–†–ê –î–ê–ù–ù–´–• –ò–ó VK")

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
    CUTOFF_DATE = get_last_update_date()

    print(f"üìÖ –°–±–æ—Ä –ø–æ—Å—Ç–æ–≤ –¥–æ: {CUTOFF_DATE}")
    print(f"üéØ –õ–∏–º–∏—Ç: {MAX_POSTS_PER_GROUP} –ø–æ—Å—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É")
    print(f"üìä –ì—Ä—É–ø–ø—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(VK_GROUPS)}")
    for group_id, group_name in VK_GROUPS.items():
        print(f"   - {group_name} ({group_id})")
    print("=" * 60)

    start_time = time.time()
    total_processed = 0

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é
    for i, (group_id, group_name) in enumerate(VK_GROUPS.items()):
        print(f"\n{'='*50}")
        print(f"üîÑ –ì—Ä—É–ø–ø–∞ {i+1}/{len(VK_GROUPS)}: {group_name}")
        print(f"{'='*50}")

        processed = collect_group_posts_optimized(group_id, group_name, CUTOFF_DATE)
        total_processed += processed

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
        if i < len(VK_GROUPS) - 1:
            time.sleep(0.5)

    total_time = time.time() - start_time

    print(f"\n{'='*60}")
    print("üéâ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê –î–ê–ù–ù–´–•")
    print(f"{'='*60}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {error_count}")
    print(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫")

    if total_processed > 0:
        speed = total_processed / total_time
        print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} –ø–æ—Å—Ç–æ–≤/—Å–µ–∫")
        print(f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {speed * 60:.1f} –ø–æ—Å—Ç–æ–≤/–º–∏–Ω")

if __name__ == "__main__":
    main()