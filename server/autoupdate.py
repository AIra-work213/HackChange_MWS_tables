# -*- coding: utf-8 -*-
"""–∞–≤—Ç–æ–æ–±–Ω–æ–≤–∞

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hk_1HcQ4Z7yOjMQF-1f0gDhrqw4qS9RO
"""

import requests
import pandas as pd
import numpy as np
from tqdm import tqdm
import time
import logging
from datetime import datetime, timedelta
import warnings
import concurrent.futures
from threading import Lock
import random

# –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore', category=FutureWarning)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.WARNING)

# ID –≥—Ä—É–ø–ø –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Å–≤—è–∑–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
VK_GROUPS = [
    '-8458649',   # –ú–¢–°
    '-26514504',  # –ë–∏–ª–∞–π–Ω
    '-18098621',  # –¢–µ–ª–µ2
    '-50353992',  # –ô–æ—Ç–∞
    '-3785'       # –ú–µ–≥–∞—Ñ–æ–Ω
]

VK_TOKEN = '954e97ed954e97ed954e97ed2d967306909954e954e97edfc65b931ccd5ae0f3c2a7afe'

# MWS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
MWS_URL = "https://tables.mws.ru/fusion/v1/datasheets/dstCNkL7G9iYsD0LY9/records"
MWS_HEADERS = {
    "Authorization": "Bearer uskSID2MFKEnL7AVNUdLrnn",
    "Content-Type": "application/json"
}
MWS_PARAMS = {
    "viewId": "viwYvxvon7TBU",
    "fieldKey": "name"
}

# –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É
MAX_POSTS_PER_GROUP = 8000

# –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
LAST_UPDATE_RECORD_ID = "last_update_tracker"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
success_count = 0
error_count = 0
counter_lock = Lock()

class VKDataCollector:
    def __init__(self, vk_token, api_version='5.199'):
        self.vk_token = vk_token
        self.api_version = api_version
        self.base_url = 'https://api.vk.com/method/'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def get_wall_posts(self, owner_id, offset=0, count=100, start_time=None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ —Å–æ —Å—Ç–µ–Ω—ã"""
        time.sleep(0.1)

        method = 'wall.get'
        params = {
            'access_token': self.vk_token,
            'owner_id': owner_id,
            'v': self.api_version,
            'offset': offset,
            'count': count,
            'extended': 0
        }

        if start_time:
            params['start_time'] = int(start_time.timestamp())

        try:
            response = self.session.get(f"{self.base_url}{method}", params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            if 'error' in data:
                error_code = data['error'].get('error_code')
                if error_code in [6, 9, 29]:
                    time.sleep(2)
                return None

            return data

        except Exception:
            return None

    def calculate_er(self, likes, reposts, comments, views):
        if not views or views == 0:
            return 0
        return ((likes + reposts + comments) / views) * 100

    def categorize_efficiency(self, er):
        if er >= 8:
            return "–í—ã—Å–æ–∫–∞—è"
        elif er >= 4:
            return "–°—Ä–µ–¥–Ω—è—è"
        elif er >= 2:
            return "–ù–∏–∑–∫–∞—è"
        else:
            return "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"

    def extract_post_data(self, post):
        try:
            post_id = post.get('id')
            owner_id = post.get('owner_id')
            date_timestamp = post.get('date')
            text = post.get('text', '')

            likes = post.get('likes', {}).get('count', 0)
            reposts = post.get('reposts', {}).get('count', 0)
            comments = post.get('comments', {}).get('count', 0)
            views = post.get('views', {}).get('count') if post.get('views') else 0

            title = ''
            for attachment in post.get('attachments', [])[:2]:
                attachment_type = attachment.get('type')
                if attachment_type in ['link', 'video', 'photo']:
                    attachment_data = attachment.get(attachment_type, {})
                    title = attachment_data.get('title', '')[:300]
                    break

            er = self.calculate_er(likes, reposts, comments, views)
            efficiency = self.categorize_efficiency(er)

            return {
                'id_group': owner_id,
                'id_post': post_id,
                'date_time': date_timestamp,
                'title': title,
                'text': text[:1000],
                'views': views,
                'likes': likes,
                'reposts': reposts,
                'comments': comments,
                'ER': round(er, 2),
                'Efficiency': efficiency,
                'day_of_week': ["–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–í—Ç–æ—Ä–Ω–∏–∫", "–°—Ä–µ–¥–∞", "–ß–µ—Ç–≤–µ—Ä–≥", "–ü—è—Ç–Ω–∏—Ü–∞", "–°—É–±–±–æ—Ç–∞", "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ"][datetime.fromtimestamp(date_timestamp).weekday()],
                'date': datetime.fromtimestamp(date_timestamp).strftime('%Y-%m-%d'),
                'time_period': self.get_time_period(date_timestamp),
                'len_text': len(text)
            }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–∞: {e}")
            return None

    def get_time_period(self, timestamp):
        hour = datetime.fromtimestamp(timestamp).hour
        if 6 <= hour < 12:
            return "–£—Ç—Ä–æ"
        elif 12 <= hour < 18:
            return "–î–µ–Ω—å"
        elif 18 <= hour < 24:
            return "–í–µ—á–µ—Ä"
        else:
            return "–ù–æ—á—å"

def get_last_update_time():
    try:
        params = {
            **MWS_PARAMS,
            "pageSize": 1,
            "filter": f'{{"operator":"and","conditions":[{{"field":"post_id","operator":"is","value":"{LAST_UPDATE_RECORD_ID}"}}]}}'
        }

        response = requests.get(
            MWS_URL,
            params=params,
            headers=MWS_HEADERS,
            timeout=10
        )

        if response.status_code == 200:
            data = response.json()
            records = data.get('data', {}).get('records', [])
            if records:
                last_update_timestamp = records[0]['fields'].get('post_date_time', 0) // 1000
                last_update_time = datetime.fromtimestamp(last_update_timestamp)
                print(f"üìÖ –ù–∞–π–¥–µ–Ω–æ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {last_update_time}")
                return last_update_time

        # –ï—Å–ª–∏ –∑–∞–ø–∏—Å–∏ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–µ–Ω—å —Å—Ç–∞—Ä–æ–µ –≤—Ä–µ–º—è
        very_old_time = datetime.fromtimestamp(0)
        print(f"üÜï –ó–∞–ø–∏—Å—å –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞—á–∞–ª–∞")
        return very_old_time

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
        very_old_time = datetime.fromtimestamp(0)
        return very_old_time

def update_last_update_time():
    current_time = datetime.now()

    record_data = {
        "records": [{
            "fields": {
                "post_id": LAST_UPDATE_RECORD_ID,
                "title": "LAST_UPDATE_TRACKER",
                "text": f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {current_time}",
                "attachment_description": "–°–∏—Å—Ç–µ–º–Ω–∞—è –∑–∞–ø–∏—Å—å",
                "views": 0,
                "likes": 0,
                "reposts": 0,
                "comments_count": 0,
                "comments": "–°–∏—Å—Ç–µ–º–Ω–∞—è –∑–∞–ø–∏—Å—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è",
                "post_date_time": int(current_time.timestamp()) * 1000,
                "owner_id": "system",
                "ER": 0,
                "Efficiency": "–°–∏—Å—Ç–µ–º–Ω–∞—è",
                "day_of_week": "–°–∏—Å—Ç–µ–º–Ω–∞—è",
                "post_date": int(current_time.timestamp()) * 1000,
                "time_period": "–°–∏—Å—Ç–µ–º–Ω–æ–µ",
                "len_text": 0
            }
        }],
        "fieldKey": "name"
    }

    try:
        params = {
            **MWS_PARAMS,
            "recordIds": LAST_UPDATE_RECORD_ID
        }

        response = requests.put(
            MWS_URL,
            params=params,
            headers=MWS_HEADERS,
            json=record_data,
            timeout=10
        )

        if response.status_code not in [200, 201]:
            response = requests.post(
                MWS_URL,
                params=MWS_PARAMS,
                headers=MWS_HEADERS,
                json=record_data,
                timeout=10
            )

        if response.status_code in [200, 201]:
            print(f"‚úÖ –í—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞: {current_time}")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {response.status_code}")
            return False

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏: {e}")
        return False

def upload_batch_to_mws(posts_data):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –ø–æ—Å—Ç–æ–≤ –≤ MWS"""
    global success_count, error_count

    if not posts_data:
        return 0

    records = []
    for post in posts_data:
        records.append({
            "post_id": str(post['id_post']),
            "title": str(post['title']),
            "text": str(post['text']),
            "attachment_description": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            "views": int(post['views']),
            "likes": int(post['likes']),
            "reposts": int(post['reposts']),
            "comments_count": int(post['comments']),
            "comments": "–ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤",
            "post_date_time": int(post['date_time']) * 1000,
            "owner_id": str(post['id_group']),
            "ER": float(post['ER']),
            "Efficiency": str(post['Efficiency']),
            "day_of_week": str(post['day_of_week']),
            "post_date": int(post['date_time']) * 1000,
            "time_period": str(post['time_period']),
            "len_text": int(post['len_text'])
        })

    data = {
        "records": [{'fields': record} for record in records],
        "fieldKey": "name"
    }

    try:
        response = requests.post(
            MWS_URL,
            params=MWS_PARAMS,
            headers=MWS_HEADERS,
            json=data,
            timeout=15
        )

        if response.status_code in [200, 201]:
            with counter_lock:
                success_count += len(posts_data)
            return len(posts_data)
        else:
            # Fallback –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
            successful = 0
            for post in posts_data:
                if upload_single_post(post):
                    successful += 1
            return successful

    except Exception:
        successful = 0
        for post in posts_data:
            if upload_single_post(post):
                successful += 1
        return successful

def upload_single_post(post_data):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ –≤ MWS"""
    global success_count, error_count

    try:
        record = {
            "post_id": str(post_data['id_post']),
            "title": str(post_data['title']),
            "text": str(post_data['text']),
            "attachment_description": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            "views": int(post_data['views']),
            "likes": int(post_data['likes']),
            "reposts": int(post_data['reposts']),
            "comments_count": int(post_data['comments']),
            "comments": "–ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤",
            "post_date_time": int(post_data['date_time']) * 1000,
            "owner_id": str(post_data['id_group']),
            "ER": float(post_data['ER']),
            "Efficiency": str(post_data['Efficiency']),
            "day_of_week": str(post_data['day_of_week']),
            "post_date": int(post_data['date_time']) * 1000,
            "time_period": str(post_data['time_period']),
            "len_text": int(post_data['len_text'])
        }

        data = {
            "records": [{'fields': record}],
            "fieldKey": "name"
        }

        response = requests.post(
            MWS_URL,
            params=MWS_PARAMS,
            headers=MWS_HEADERS,
            json=data,
            timeout=10
        )

        if response.status_code in [200, 201]:
            with counter_lock:
                success_count += 1
            return True
        else:
            error_text = response.text.lower()
            if "duplicate" in error_text or "—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç" in error_text:
                with counter_lock:
                    success_count += 1
                return True
            return False

    except Exception:
        with counter_lock:
            error_count += 1
        return False

def collect_new_posts_from_group(owner_id, last_update_time):
    """–°–±–æ—Ä –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    global success_count, error_count

    collector = VKDataCollector(VK_TOKEN)

    print(f"–ì—Ä—É–ø–ø–∞ {owner_id}: –ø–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –ø–æ—Å–ª–µ {last_update_time}")

    # –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    total_new_posts = 0
    offset = 0
    has_more_posts = True

    while has_more_posts and total_new_posts < MAX_POSTS_PER_GROUP:
        wall_data = collector.get_wall_posts(owner_id, offset, 100, last_update_time)
        if not wall_data:
            break

        posts = wall_data['response']['items']
        if not posts:
            break

        for post in posts:
            post_timestamp = post.get('date', 0)
            post_time = datetime.fromtimestamp(post_timestamp)
            if post_time > last_update_time and total_new_posts < MAX_POSTS_PER_GROUP:
                total_new_posts += 1
            else:
                has_more_posts = False
                break

        if has_more_posts:
            offset += 100

    if total_new_posts == 0:
        print(f"–ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return 0

    limited_new_posts = min(total_new_posts, MAX_POSTS_PER_GROUP)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {total_new_posts} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤")
    print(f"   –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {limited_new_posts} –ø–æ—Å—Ç–æ–≤")

    # –°–æ–±–∏—Ä–∞–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
    offset = 0
    processed_count = 0
    has_more_posts = True

    with tqdm(total=limited_new_posts, desc=f"–ì—Ä—É–ø–ø–∞ {owner_id}") as pbar:
        while has_more_posts and processed_count < limited_new_posts:
            wall_data = collector.get_wall_posts(owner_id, offset, 100, last_update_time)
            if not wall_data:
                break

            posts = wall_data['response']['items']
            if not posts:
                break

            batch_posts = []
            for post in posts:
                post_timestamp = post.get('date', 0)
                post_time = datetime.fromtimestamp(post_timestamp)
                if post_time > last_update_time and processed_count < limited_new_posts:
                    post_data = collector.extract_post_data(post)
                    if post_data:
                        batch_posts.append(post_data)
                    processed_count += 1
                    pbar.update(1)
                else:
                    has_more_posts = False
                    break

            if batch_posts:
                upload_batch_to_mws(batch_posts)

            offset += 100
            time.sleep(0.3)

    print(f"‚úÖ –ì—Ä—É–ø–ø–∞ {owner_id}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤")
    return processed_count

def main_incremental():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    global success_count, error_count

    print("–ó–ê–ü–£–°–ö –ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–û–ì–û –û–ë–ù–û–í–õ–ï–ù–ò–Ø")
    print("–†–µ–∂–∏–º: —Å–±–æ—Ä —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤")
    print(f"–õ–∏–º–∏—Ç: {MAX_POSTS_PER_GROUP} –ø–æ—Å—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É")
    print("=" * 60)

    # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    last_update_time = get_last_update_time()

    start_time = time.time()

    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏
    success_count = 0
    error_count = 0

    total_processed = 0

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã
    for i, group_id in enumerate(VK_GROUPS):
        print(f"\n{'='*50}")
        print(f"–ì—Ä—É–ø–ø–∞ {i+1}/{len(VK_GROUPS)}: {group_id}")
        print(f"{'='*50}")

        processed = collect_new_posts_from_group(group_id, last_update_time)
        total_processed += processed

        if i < len(VK_GROUPS) - 1:
            time.sleep(1)

    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±—ã–ª–∏ –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã
    if success_count > 0:
        print(f"\n–û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è...")
        update_last_update_time()

    total_time = time.time() - start_time

    print(f"\n{'='*60}")
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*60}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {error_count}")
    print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫")

    if success_count > 0:
        print(f"\n–ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {success_count} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤")
    else:
        print(f"\n‚Ñπ–ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

if __name__ == "__main__":
    main_incremental()